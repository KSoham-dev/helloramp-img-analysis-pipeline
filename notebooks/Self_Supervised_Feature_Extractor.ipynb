{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a903300d-2c9e-4eb2-838f-30fb0e57615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a690903-e8ba-49df-9ea1-d57c4790f20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57 images in car-img-hr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/soham/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 0.062256\n",
      "Epoch 10: 0.031049\n",
      "Epoch 15: 0.019045\n",
      "Epoch 20: 0.014042\n",
      "Epoch 5: 0.051431\n",
      "Epoch 10: 0.041459\n",
      "Epoch 15: 0.031801\n",
      "Epoch 20: 0.023122\n",
      "Epoch 25: 0.018062\n",
      "Epoch 30: 0.013927\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "class UnsupervisedFineTuner(nn.Module):\n",
    "    def __init__(self, encoder_type='dinov2', freeze=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        if encoder_type == 'dinov2':\n",
    "            self.encoder = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "            dim = 768\n",
    "        else:\n",
    "            from efficientnet_pytorch import EfficientNet\n",
    "            self.encoder = EfficientNet.from_pretrained('efficientnet-b5')\n",
    "            dim = 2048\n",
    "            \n",
    "        if freeze:\n",
    "            for p in self.encoder.parameters(): p.requires_grad = False\n",
    "        \n",
    "        self.encoder_type = encoder_type\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(dim, 512*7*7), nn.ReLU(), nn.Unflatten(1, (512,7,7)),\n",
    "            nn.ConvTranspose2d(512,256,3,2,1,1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256,128,3,2,1,1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128,64,3,2,1,1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64,32,3,2,1,1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32,3,3,2,1,1), nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        if self.encoder_type == 'dinov2':\n",
    "            return self.encoder(x)\n",
    "        f = self.encoder.extract_features(x)\n",
    "        return F.adaptive_avg_pool2d(f, 1).flatten(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        f = self.encode(x)\n",
    "        return self.decoder(f), f\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, paths, transform):\n",
    "        self.paths, self.transform = paths, transform\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, i):\n",
    "        return self.transform(Image.open(self.paths[i]).convert('RGB'))\n",
    "\n",
    "def train(model, loader, epochs, lr, device, freeze_enc=True):\n",
    "    model.to(device)\n",
    "    params = model.decoder.parameters() if freeze_enc else model.parameters()\n",
    "    opt = torch.optim.Adam(params, lr=lr)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        total = 0\n",
    "        for imgs in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            recon, _ = model(imgs)\n",
    "            loss = F.mse_loss(recon, imgs)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "        if (e+1) % 5 == 0: print(f\"Epoch {e+1}: {total/len(loader):.6f}\")\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Load all images from car-img-hr directory\n",
    "    img_dir = 'car-img-hr'\n",
    "    paths = glob(f'{img_dir}/*.jpg') + glob(f'{img_dir}/*.png') + \\\n",
    "            glob(f'{img_dir}/*.jpeg') + glob(f'{img_dir}/*.JPG')\n",
    "    print(f\"Found {len(paths)} images in {img_dir}\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    loader = DataLoader(UnlabeledDataset(paths, transform), \n",
    "                       batch_size=16, shuffle=True, num_workers=4)\n",
    "    \n",
    "    model = UnsupervisedFineTuner('dinov2', freeze=True)\n",
    "    model = train(model, loader, 20, 1e-3, device, freeze_enc=True)\n",
    "    \n",
    "    for p in model.encoder.parameters(): p.requires_grad = True\n",
    "    model = train(model, loader, 30, 1e-4, device, freeze_enc=False)\n",
    "    \n",
    "    torch.save(model.encoder.state_dict(), 'encoder.pth')\n",
    "    return model\n",
    "\n",
    "def extract_features(model, img_dir='car-img-hr', device='cuda'):\n",
    "    paths = glob(f'{img_dir}/*.jpg') + glob(f'{img_dir}/*.png') + \\\n",
    "            glob(f'{img_dir}/*.jpeg') + glob(f'{img_dir}/*.JPG')\n",
    "    \n",
    "    model.eval().to(device)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    features = []\n",
    "    for i in range(0, len(paths), 32):\n",
    "        batch = torch.stack([transform(Image.open(p).convert('RGB')) \n",
    "                            for p in paths[i:i+32]]).to(device)\n",
    "        with torch.no_grad():\n",
    "            features.append(model.encode(batch).cpu().numpy())\n",
    "    return np.vstack(features)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = main()\n",
    "    features = extract_features(model)\n",
    "    \n",
    "    # Train classifier on extracted features\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    # MLPClassifier((256,128), max_iter=500).fit(features, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7645c24-7c0c-4421-8cd9-72dfb2d6ed73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/soham/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m         features = model(batch)\n\u001b[32m     29\u001b[39m         all_features.append(features.cpu().numpy())\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m features_array = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFeatures shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeatures_array.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/helloramp_int/object_classification/.venv/lib/python3.13/site-packages/numpy/_core/shape_base.py:292\u001b[39m, in \u001b[36mvstack\u001b[39m\u001b[34m(tup, dtype, casting)\u001b[39m\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    291\u001b[39m     arrs = (arrs,)\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Get images\n",
    "image_paths = list(Path('path/to/images').glob('*.jpg'))\n",
    "\n",
    "# Extract features\n",
    "all_features = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(image_paths), 32):\n",
    "        batch_paths = image_paths[i:i+32]\n",
    "        batch = torch.stack([transform(Image.open(p).convert('RGB')) for p in batch_paths]).to(device)\n",
    "        features = model(batch)\n",
    "        all_features.append(features.cpu().numpy())\n",
    "\n",
    "features_array = np.vstack(all_features)\n",
    "print(f\"Features shape: {features_array.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dac196-26a2-4bc8-b694-907e8a0d9a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
